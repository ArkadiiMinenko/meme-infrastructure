# üöÄ Step 1 ‚Äî Infrastructure Foundation  
**Terraform √ó Proxmox √ó Reality Check**

---

## Context

This project is a DevOps homelab built with one clear intention:  
to design infrastructure that behaves like **production**, not like a demo.

No managed services.  
No hidden automation.  
Full control over virtualization, networking, and provisioning.

Before touching Kubernetes, GitOps, or CI/CD, the foundation has to be solid.

This step is about building that foundation.

---

## üéØ Goals of Step 1

- Understand how Proxmox actually manages VM lifecycle
- Set up Terraform locally as the **single source of truth** for infrastructure
- Eliminate manual VM creation and UI-driven configuration
- Prepare a clean base for future Kubernetes nodes

This step was less about speed ‚Äî  
and more about learning the platform‚Äôs edges.

---

## üß± What Was Built

### Proxmox Side

- Imported an Ubuntu **cloud-init** image
- Converted it into a reusable **VM template**

Verified:
- disk layout and boot mode (SeaBIOS)
- virtio storage and networking
- cloud-init compatibility

---

### Terraform Side

- Configured Terraform to control Proxmox via API
- Described VM configuration entirely in code:
  - CPU and RAM limits
  - static IPs via cloud-init
  - SSH access using public keys
- Ensured infrastructure could be recreated from scratch at any time

At this point, the goal was simple:

> **`terraform apply` should produce a usable VM ‚Äî every time.**

---

## ‚ö†Ô∏è What Went Wrong (and Why It Mattered)

### 1. Terraform Hanging on `Still creating...`

Terraform would wait indefinitely, even though the VM was already running.

**Why?**  
Terraform relies on **QEMU Guest Agent** to receive the VM‚Äôs IP address and mark it as ‚Äúready‚Äù.

No agent ‚Üí no signal ‚Üí Terraform waits forever.

**Fix:**
- Installed and enabled `qemu-guest-agent` in the template
- Enabled agent support in Proxmox

---

### 2. PXE Boot Instead of Disk Boot

Some VMs didn‚Äôt boot at all and dropped straight into PXE.

**Symptoms:**
- `Boot failed: could not read the boot disk`
- Root disk attached as `unused0`
- `scsi0` missing
- cloud-init CDROM not connected

**What actually happened:**  
The Terraform provider created the disk, but failed to attach it correctly.

Result:  
A VM that exists ‚Äî but cannot boot.

Temporary fixes were possible via `qm set`, but‚Ä¶

---

### 3. The Real Issue: Provider Instability

The initial Proxmox Terraform provider showed:
- inconsistent VM definitions
- provider crashes
- partial resource creation
- repeated manual intervention

At this point, the problem was no longer configuration ‚Äî  
it was **tool reliability**.

---

## üß† Engineering Decision

Instead of stacking workarounds and post-create scripts, the decision was made to:

> **Replace the provider.**

Sometimes the most correct solution is not a clever workaround,  
but choosing a tool that behaves predictably.

This immediately stabilized the provisioning process.

---

## ‚úÖ Outcome of Step 1

- Terraform provisions Proxmox VMs reliably
- No manual fixes required after creation
- VM state is fully reproducible from code
- Infrastructure is ready for Kubernetes bootstrap

Most importantly:  
**the foundation is now trusted.**

---

## üìå Key Takeaways

- Infrastructure as Code does not hide platform behavior ‚Äî it reveals it
- Minimal cloud images require deliberate preparation
- Terraform is only as reliable as the signals it receives
- Knowing *when to switch tools* is as important as knowing how to configure them

# üöÄ Step 2 ‚Äî Configuration Management & Kubernetes Bootstrapping

**Terraform √ó Ansible √ó Kubernetes (Zero-Touch Deployment)**

---

## Context

After virtual machines were provisioned with Terraform, the infrastructure was still incomplete.

Bare VMs are not a platform.

This step focuses on **turning raw compute into a working Kubernetes cluster**,  
without manual SSH access or node-by-node configuration.

The goal is full automation from infrastructure provisioning  
to a ready-to-use Kubernetes control plane.

---

## üéØ Goals of Step 2

- Automate OS preparation for Kubernetes nodes
- Eliminate manual SSH access during cluster setup
- Bootstrap a Kubernetes cluster using declarative configuration
- Connect Terraform (infrastructure) and Ansible (configuration) into a single workflow
- Achieve repeatable, zero-touch cluster deployment

---

## üß± What Was Built

### Terraform ‚Üî Ansible Integration

- Terraform dynamically generates `inventory.ini`
- No hardcoded IP addresses
- Nodes are automatically grouped into:
  - `[master]`
  - `[workers]`
- Ansible becomes infrastructure-aware without manual updates

Result:

> Infrastructure changes automatically propagate into configuration management.

---

### OS Preparation (Ansible)

A dedicated Ansible playbook prepares all nodes for Kubernetes:

- Disabled SWAP (runtime and persistent)
- Loaded required kernel modules:
  - `overlay`
  - `br_netfilter`
- Configured `sysctl` parameters for Kubernetes networking
- Installed and configured container runtime (`containerd`)
- Installed Kubernetes components:
  - `kubeadm`
  - `kubelet`
  - `kubectl`

All steps are idempotent and safe to re-run.

---

### Kubernetes Cluster Bootstrapping

Ansible automates the entire cluster lifecycle:

- Initializes the Control Plane using `kubeadm`
- Installs CNI networking (Flannel)
- Generates and distributes the join token
- Automatically joins Worker nodes to the cluster
- Configures kubeconfig access for cluster management

The cluster reaches a ready state without any manual intervention.

---

## ‚ö†Ô∏è What Went Wrong (and Why It Mattered)

### 1. SSH Access Failing on a Single Node

One node refused SSH connections with `Connection refused`,  
while network connectivity (ping) was fully functional.

**Initial assumption:**  
Cloud-init or OS configuration issue.

**Actual cause:**  
IP address conflict inside the home network.

The IP was already silently assigned to an IoT device.

**Fix:**

- Adjusted Terraform IP allocation logic
- Moved Kubernetes nodes into a dedicated address range

---

### 2. Kubernetes Reset Artifacts

After reinitializing the cluster, pods were stuck in `ContainerCreating`.

**Root cause:**

- Old CNI interfaces (`cni0`) persisted after `kubeadm reset`
- Network conflicts prevented pod networking

**Fix:**

- Explicit cleanup of CNI interfaces before re-initialization
- Ensured clean network state before cluster bootstrap

---

## üß† Engineering Decisions

- Terraform handles **infrastructure**
- Ansible handles **configuration**
- No configuration logic inside Terraform
- No infrastructure assumptions inside Ansible

Each tool does one job ‚Äî and does it well.

---

## ‚úÖ Outcome of Step 2

- Fully automated Kubernetes cluster provisioning
- Zero manual SSH configuration
- Infrastructure and configuration are fully reproducible
- Cluster can be destroyed and recreated reliably

```text
terraform apply
ansible-playbook
‚Üí Kubernetes cluster ready
